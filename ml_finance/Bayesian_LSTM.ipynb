{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Bayesian LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9de99a5c409b4544"
  },
  {
   "cell_type": "markdown",
   "source": [
    "import library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b59cf2a7d1ffdab9"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-22T16:19:31.761417Z",
     "start_time": "2024-03-22T16:18:52.472365Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from blitz.modules import BayesianLSTM\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "start_date = '1980-01-01'\n",
    "end_date = '2024-03-23'\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(\n",
    "    ticker, start = start_date, end = end_date\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T16:21:33.356547Z",
     "start_time": "2024-03-22T16:21:30.061282Z"
    }
   },
   "id": "6942ec0f5ead71b1",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Date\n1980-12-12      0.128348\n1980-12-15      0.121652\n1980-12-16      0.112723\n1980-12-17      0.115513\n1980-12-18      0.118862\n                 ...    \n2024-03-18    173.720001\n2024-03-19    176.080002\n2024-03-20    178.669998\n2024-03-21    171.369995\n2024-03-22    172.509995\nName: Close, Length: 10910, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Close']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T16:24:54.364300Z",
     "start_time": "2024-03-22T16:24:54.340676Z"
    }
   },
   "id": "d8d0e6beccd1c36f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ret = data['Close'].pct_change().dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T16:27:52.116422Z",
     "start_time": "2024-03-22T16:27:52.107875Z"
    }
   },
   "id": "1bfb4dd8f34497c2",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_timestamps_ds(series, timestep_size):\n",
    "    time_stamps = []\n",
    "    labels = []\n",
    "    aux_deque = deque(maxlen = timestep_size)\n",
    "    \n",
    "    #starting the timestep deque\n",
    "    for i in range(timestep_size):\n",
    "        aux_deque.append(0)\n",
    "    \n",
    "    #feed the timestamps list\n",
    "    for i in range(len(series)-1):\n",
    "        aux_deque.append(series[i])\n",
    "        time_stamps.append(list(aux_deque))\n",
    "    \n",
    "    #feed the labels lsit\n",
    "    for i in range(len(series)-1):\n",
    "        labels.append(series[i + 1])\n",
    "    \n",
    "    assert len(time_stamps) == len(labels), \"Something went wrong\"\n",
    "    \n",
    "    #torch-tensoring it\n",
    "    features = torch.tensor(time_stamps[timestep_size:]).float()\n",
    "    labels = torch.tensor(labels[timestep_size:]).float()\n",
    "    \n",
    "    return features, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T16:24:42.636915Z",
     "start_time": "2024-03-22T16:24:42.624264Z"
    }
   },
   "id": "8dde1ef3f3f7fed9",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@variational_estimator\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.lstm_1 = BayesianLSTM(1, 10)\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_, _ = self.lstm_1(x)\n",
    "        \n",
    "        #gathering only the latent end-of-sequence for the linear layer\n",
    "        x_ = x_[:, -1, :]\n",
    "        x_ = self.linear(x_)\n",
    "        return x_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T16:27:12.857344Z",
     "start_time": "2024-03-22T16:27:12.840125Z"
    }
   },
   "id": "eb14a04f6e98c64a",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Xs, ys = create_timestamps_ds(ret, timestep_size = 21)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size = .05, shuffle = False)\n",
    "\n",
    "ds = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(ds, batch_size = 1, shuffle = False)\n",
    "\n",
    "net = NN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T17:21:54.562247Z",
     "start_time": "2024-03-22T17:21:54.432036Z"
    }
   },
   "id": "3376010a1117e254",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [47:19<00:00, 283.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "iteration = 0\n",
    "for epoch in tqdm(range(10)):\n",
    "    for i, (datapoints, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        datapoints = datapoints.reshape(1, 21, 1)\n",
    "\n",
    "        loss = net.sample_elbo(\n",
    "            inputs = datapoints, \n",
    "            labels = labels, \n",
    "            criterion = criterion, \n",
    "            sample_nbr = 3\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T18:09:17.510561Z",
     "start_time": "2024-03-22T17:21:57.556932Z"
    }
   },
   "id": "c08c0e3698357d49",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a9df768c0d149310"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability.python.distributions import Normal\n",
    "\n",
    "def compute_KL_univariate_prior(univariateprior, theta, sample):\n",
    "    \"\"\"\n",
    "    :param prior:  assuming univariate prior of Normal(m,s);\n",
    "    :param posterior: (theta: mean,std) to create posterior q(w/theta) i.e. Normal(mean,std)\n",
    "    :param sample: Number of sample\n",
    "    \"\"\"\n",
    "    sample=tf.reshape(sample, [-1])  #flatten vector\n",
    "    (mean,std)=theta\n",
    "    mean =tf.reshape(mean, [-1])\n",
    "    std=tf.reshape(std, [-1])\n",
    "    posterior = Normal(mean, std)\n",
    "    (mean2,std2) = univariateprior\n",
    "    prior=Normal(mean2, std2)\n",
    "\n",
    "    q_theta=tf.reduce_sum(posterior.log_prob(sample))\n",
    "    p_d=tf.reduce_sum(prior.log_prob(sample))\n",
    "\n",
    "    KL=tf.subtract(q_theta,p_d)\n",
    "    return KL\n",
    "\n",
    "def variationalPosterior(shape, name, prior, istraining):\n",
    "    \"\"\"\n",
    "    this function create a variational posterior q(w/theta) over a given \"weight:w\" of the network\n",
    "    theta is parameterized by mean+standard*noise we apply the reparameterization trick from kingma et al, 2014\n",
    "    with correct loss function (free energy) we learn mean and standard to estimate of theta, thus can estimate\n",
    "    posterior p(w/D) by computing KL loss for each variational posterior q(w/theta) with prior(w)\n",
    "\n",
    "    :param name: is the name of the tensor/variable to create variational posterior  q(w/Q) for true posterior (p(w/D))\n",
    "    :param shape: is the shape of the weight variable\n",
    "    :param training: whether in training or inference mode\n",
    "    :return: samples (i.e. weights), mean of weights, std in-case of the training there is noise add to the weights\n",
    "    \"\"\"\n",
    "    # theta=mu+sigma i.e. theta = mu+sigma i.e. mu+log(1+exp(rho)), log(1+exp(rho))\n",
    "    # is the computed by using tf.math.softplus(rho)\n",
    "    mu=tf.get_variable(\"{}_mean\".format(name), shape=shape, dtype=tf.float32);\n",
    "    rho=tf.get_variable(\"{}_rho\".format(name), shape=shape, dtype=tf.float32);\n",
    "    sigma = tf.math.softplus(rho)\n",
    "\n",
    "    #if training we add noise to variation parameters theta\n",
    "    if (istraining):\n",
    "        epsilon= Normal(0,1.0).sample(shape)\n",
    "        sample=mu+sigma*epsilon\n",
    "    else:\n",
    "        sample=mu+sigma;\n",
    "\n",
    "    theta=(mu,sigma)\n",
    "\n",
    "    kl_loss = compute_KL_univariate_prior(prior, theta, sample)\n",
    "\n",
    "    tf.summary.histogram(name + '_rho_hist', rho)\n",
    "    tf.summary.histogram(name + '_mu_hist', mu)\n",
    "    tf.summary.histogram(name + '_sigma_hist', sigma)\n",
    "\n",
    "    # we shall used this in the training to get kl loss\n",
    "    tf.add_to_collection(\"KL_layers\", kl_loss)\n",
    "\n",
    "    return sample, mu, sigma"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "777755ff8712246e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class BayesianLSTMCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units, prior_fn, is_training, **kwargs):\n",
    "        super(BayesianLSTMCell, self).__init__(**kwargs)\n",
    "        self.num_units = num_units\n",
    "        self.prior_fn = prior_fn\n",
    "        self.is_training = is_training\n",
    "        self.state_size = self.num_units\n",
    "        self.output_size = self.num_units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(shape=(input_dim + self.num_units, 4 * self.num_units),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.num_units, 4 * self.num_units),\n",
    "                                                initializer='orthogonal',\n",
    "                                                name='recurrent_kernel')\n",
    "        self.bias = self.add_weight(shape=(4 * self.num_units,),\n",
    "                                    initializer='zeros',\n",
    "                                    name='bias')\n",
    "        # Variational posterior weights/biases\n",
    "        # Implement your variationalPosterior function based on your needs\n",
    "        # self.w, self.w_mean, self.w_sd = variationalPosterior(...)\n",
    "        # self.b, self.b_mean, self.b_sd = variationalPosterior(...)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        concat_inputs = tf.concat([inputs, prev_output], axis=-1)\n",
    "\n",
    "        gate_inputs = tf.matmul(concat_inputs, self.kernel)\n",
    "        gate_inputs = tf.nn.bias_add(gate_inputs, self.bias)\n",
    "\n",
    "        i, j, f, o = tf.split(gate_inputs, num_or_size_splits=4, axis=1)\n",
    "\n",
    "        new_cell = tf.sigmoid(f + self.recurrent_activation_bias) * states[1] + tf.sigmoid(i) * tf.tanh(j)\n",
    "        new_hidden = tf.sigmoid(o) * tf.tanh(new_cell)\n",
    "\n",
    "        return new_hidden, [new_hidden, new_cell]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b93f775a9e4c105"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
